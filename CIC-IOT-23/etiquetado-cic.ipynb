{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etiquetado CIC-IOT-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Para GREETH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/mirai-greeth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_directory = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/loss-rows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conn_log(folder_path):\n",
    "    conn_log_path = os.path.join(folder_path, \"conn_stadistics.log\")\n",
    "    \n",
    "    # Check if conn.log file exists\n",
    "    if os.path.exists(conn_log_path):\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        with open(conn_log_path, 'r') as file:\n",
    "            header_line = file.readlines()[6].strip().split('\\t')[1:]\n",
    "        df = pd.read_csv(conn_log_path, sep='\\t', skiprows=8, names=header_line, skipfooter=1, engine='python')\n",
    "        \n",
    "        print(folder_name)\n",
    "        # Check for the file with name folder_name + _loss_rows.csv  \n",
    "        loss_rows_path = os.path.join(loss_directory, f'{folder_name}_loss_rows.csv')\n",
    "        if os.path.exists(loss_rows_path):\n",
    "            print(f\"Loss rows file found: {loss_rows_path}\")\n",
    "            df_loss = pd.read_csv(loss_rows_path)\n",
    "            # Identify rows to be removed\n",
    "            rows_to_remove = df[df['uid'].isin(df_loss['uid'])]\n",
    "\n",
    "            # Print the rows that are going to be removed\n",
    "            print(\"Rows to be removed:\")\n",
    "            print(rows_to_remove)\n",
    "            # Remove rows from df where df['uid'] is in df_loss['uid']\n",
    "            df = df[~df['uid'].isin(df_loss['uid'])]\n",
    "            # Save concatenated data frame to CSV\n",
    "            output_path = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/labeled-csv/\"  # Change this to the desired directory path\n",
    "            csv_filename = os.path.join(output_path, f\"{folder_name}_labeled.csv\")\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "        else:\n",
    "            print(f\"Loss rows file not found for {folder_name}\")\n",
    "        # Once found, open that loss file as csv, look for the uids to remove them in the new df we are going to create\n",
    "    else:\n",
    "        print(f\"conn.log not found in {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mirai-greeth_flood21\n",
      "Loss rows file found: /root/bbdd/logs-zeek/cic-iot-2023-logs/loss-rows/Mirai-greeth_flood21_loss_rows.csv\n",
      "Rows to be removed:\n",
      "                   ts            startTime                 uid  \\\n",
      "415460   1.673882e+09  2023-01-16 15:06:58  C1V46D3gPulSadEHT1   \n",
      "623210   1.673882e+09  2023-01-16 15:08:31  C0ZhFL3DXwicmovUlh   \n",
      "714544   1.673881e+09  2023-01-16 15:04:34  CHQxz31dURSzn64834   \n",
      "769911   1.673882e+09  2023-01-16 15:15:01   CA0t4onOcZG1tuwPi   \n",
      "769912   1.673882e+09  2023-01-16 15:15:01  ClJpJu1UbP3Oimjnx5   \n",
      "...               ...                  ...                 ...   \n",
      "2847303  1.673883e+09  2023-01-16 15:33:17  CJ987W3tlJN8Awk5g9   \n",
      "3272415  1.673883e+09  2023-01-16 15:34:24  CYIWJV3cj1FQRXjuMg   \n",
      "3300288  1.673883e+09  2023-01-16 15:34:24   C6U3dukMMTRV7Wye7   \n",
      "3346522  1.673883e+09  2023-01-16 15:33:38   CaW0wwTILr01lsUp4   \n",
      "3346533  1.673883e+09  2023-01-16 15:33:34   CSp33ici4VpN9cTO3   \n",
      "\n",
      "               id.orig_h  id.orig_p       id.resp_h  id.resp_p proto service  \\\n",
      "415460   192.168.137.103      59523   46.51.133.159        443   tcp     ssl   \n",
      "623210    192.168.137.38      37088     52.51.76.26         80   tcp    http   \n",
      "714544   192.168.137.129      57237     17.57.147.6       5223   tcp       -   \n",
      "769911    192.168.137.68      47842    13.33.165.68        443   tcp     ssl   \n",
      "769912   192.168.137.209      38794   63.32.248.233        443   tcp     ssl   \n",
      "...                  ...        ...             ...        ...   ...     ...   \n",
      "2847303  192.168.137.206      37380  34.158.253.218         80   tcp       -   \n",
      "3272415  192.168.137.100      44881  172.217.13.106        443   tcp     ssl   \n",
      "3300288  192.168.137.100      44880  172.217.13.106        443   tcp     ssl   \n",
      "3346522   192.168.137.80      50027    52.34.171.19       8883   tcp     ssl   \n",
      "3346533   192.168.137.80      41105   44.237.63.131       8883   tcp     ssl   \n",
      "\n",
      "           duration  ... time_min   time_max orig_time_mean orig_time_std  \\\n",
      "415460     0.698808  ...      0.0   0.218961       0.049729      0.066482   \n",
      "623210     0.321153  ...      0.0   0.118322       0.052392      0.050797   \n",
      "714544   238.813861  ...      0.0  69.631833       0.327181      0.729973   \n",
      "769911     0.454936  ...      0.0   0.182771       0.029383      0.057458   \n",
      "769912     0.885440  ...      0.0   0.262175       0.080112      0.092184   \n",
      "...             ...  ...      ...        ...            ...           ...   \n",
      "2847303   41.214331  ...      0.0  11.407609       1.582556      3.391313   \n",
      "3272415    0.246156  ...      0.0   0.058655       0.025648      0.028404   \n",
      "3300288    2.187359  ...      0.0   0.168344       0.051693      0.059709   \n",
      "3346522   28.545455  ...      0.0   8.499212       1.158279       2.19779   \n",
      "3346533   82.271877  ...      0.0  60.085253       3.160351     11.587714   \n",
      "\n",
      "        orig_time_min  orig_time_max resp_time_mean  resp_time_std  \\\n",
      "415460       0.000000       0.221580       0.099457       0.109325   \n",
      "623210       0.000003       0.118322       0.104783        0.08729   \n",
      "714544       0.000001       3.751127       5.623620      16.215867   \n",
      "769911       0.000000       0.219396       0.037911       0.071398   \n",
      "769912       0.000000       0.275508       0.098382       0.094959   \n",
      "...               ...            ...            ...            ...   \n",
      "2847303      0.000000      11.407609       1.349904       3.208486   \n",
      "3272415      0.000000       0.085684       0.027351       0.036202   \n",
      "3300288      0.000000       0.200419       0.025434       0.038605   \n",
      "3346522      0.000000       8.656252       0.911193       1.597717   \n",
      "3346533      0.000000      60.085253       2.258470       9.942339   \n",
      "\n",
      "         resp_time_min  resp_time_max  \n",
      "415460        0.000154       0.322674  \n",
      "623210        0.004639       0.217366  \n",
      "714544        0.000000      69.631833  \n",
      "769911        0.000000       0.262942  \n",
      "769912        0.000000       0.262175  \n",
      "...                ...            ...  \n",
      "2847303       0.000001      11.487668  \n",
      "3272415       0.000000       0.111074  \n",
      "3300288       0.000000       0.168344  \n",
      "3346522       0.000001       7.148585  \n",
      "3346533       0.000002      60.213407  \n",
      "\n",
      "[68 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(main_directory):\n",
    "    folder_path = os.path.join(main_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_conn_log(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Para GREIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/mirai-greip\"\n",
    "binary_label = 1\n",
    "label = \"Mirai\"\n",
    "detailed_label=\"Mirai-greip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conn_log(folder_path,binary_label,label,detailed_label):\n",
    "    conn_log_path = os.path.join(folder_path, \"conn_stadistics.log\")\n",
    "    \n",
    "    # Check if conn.log file exists\n",
    "    if os.path.exists(conn_log_path):\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        with open(conn_log_path, 'r') as file:\n",
    "            header_line = file.readlines()[6].strip().split('\\t')[1:]\n",
    "        df = pd.read_csv(conn_log_path, sep='\\t', skiprows=8, names=header_line, skipfooter=1, engine='python')\n",
    "        \n",
    "        print(folder_name)\n",
    "        # Check for the file with name folder_name + _loss_rows.csv  \n",
    "        loss_rows_path = os.path.join(loss_directory, f'{folder_name}_loss_rows.csv')\n",
    "        if os.path.exists(loss_rows_path):\n",
    "            print(f\"Loss rows file found: {loss_rows_path}\")\n",
    "            df_loss = pd.read_csv(loss_rows_path)\n",
    "            # Identify rows to be removed\n",
    "            rows_to_remove = df[df['uid'].isin(df_loss['uid'])]\n",
    "\n",
    "            # Print the rows that are going to be removed\n",
    "            print(\"Rows to be removed:\")\n",
    "            print(rows_to_remove)\n",
    "            # Remove rows from df where df['uid'] is in df_loss['uid']\n",
    "            df = df[~df['uid'].isin(df_loss['uid'])]\n",
    "            df['binary-label']=binary_label\n",
    "            df['label']=label\n",
    "            df['detailed-label']=detailed_label\n",
    "            # Save concatenated data frame to CSV\n",
    "            output_path = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/labeled-csv/\"  # Change this to the desired directory path\n",
    "            csv_filename = os.path.join(output_path, f\"{folder_name}_labeled.csv\")\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "        else:\n",
    "            print(f\"Loss rows file not found for {folder_name}\")\n",
    "        # Once found, open that loss file as csv, look for the uids to remove them in the new df we are going to create\n",
    "    else:\n",
    "        print(f\"conn.log not found in {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(main_directory):\n",
    "    folder_path = os.path.join(main_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_conn_log(folder_path,binary_label,label,detailed_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Para udpplain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/mirai-udpplain\"\n",
    "binary_label = 1\n",
    "label = \"Mirai\"\n",
    "detailed_label=\"Mirai-udpplain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(main_directory):\n",
    "    folder_path = os.path.join(main_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_conn_log(folder_path,binary_label,label,detailed_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Para ddos-http-flood**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/\"\n",
    "binary_label = 1\n",
    "label = \"Mirai\"\n",
    "detailed_label=\"Mirai-greip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conn_log(folder_path,binary_label,label,detailed_label):\n",
    "    conn_log_path = os.path.join(folder_path, \"conn_stadistics.log\")\n",
    "    \n",
    "    # Check if conn.log file exists\n",
    "    if os.path.exists(conn_log_path):\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "        with open(conn_log_path, 'r') as file:\n",
    "            header_line = file.readlines()[6].strip().split('\\t')[1:]\n",
    "        df = pd.read_csv(conn_log_path, sep='\\t', skiprows=8, names=header_line, skipfooter=1, engine='python')\n",
    "        \n",
    "        print(folder_name)\n",
    "        # Check for the file with name folder_name + _loss_rows.csv  \n",
    "        loss_rows_path = os.path.join(loss_directory, f'{folder_name}_loss_rows.csv')\n",
    "        if os.path.exists(loss_rows_path):\n",
    "            print(f\"Loss rows file found: {loss_rows_path}\")\n",
    "            df_loss = pd.read_csv(loss_rows_path)\n",
    "            # Identify rows to be removed\n",
    "            rows_to_remove = df[df['uid'].isin(df_loss['uid'])]\n",
    "\n",
    "            # Print the rows that are going to be removed\n",
    "            print(\"Rows to be removed:\")\n",
    "            print(rows_to_remove)\n",
    "            # Remove rows from df where df['uid'] is in df_loss['uid']\n",
    "            df = df[~df['uid'].isin(df_loss['uid'])]\n",
    "            df['binary-label']=binary_label\n",
    "            df['label']=label\n",
    "            df['detailed-label']=detailed_label\n",
    "            # Save concatenated data frame to CSV\n",
    "            output_path = \"/root/bbdd/logs-zeek/cic-iot-2023-logs/labeled-csv/\"  # Change this to the desired directory path\n",
    "            csv_filename = os.path.join(output_path, f\"{folder_name}_labeled.csv\")\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "        else:\n",
    "            print(f\"Loss rows file not found for {folder_name}\")\n",
    "        # Once found, open that loss file as csv, look for the uids to remove them in the new df we are going to create\n",
    "    else:\n",
    "        print(f\"conn.log not found in {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(main_directory):\n",
    "    folder_path = os.path.join(main_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_conn_log(folder_path,binary_label,label,detailed_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
